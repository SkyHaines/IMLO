{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOq4up1B6YnC7PuphKItIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SkyHaines/IMLO/blob/main/Coursework%3F%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR 10 dataset contains 60,000 3x32x32 colour images, with 10 classes."
      ],
      "metadata": {
        "id": "-84D_fD6rDLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Need to look into how this works, perhaps i could tweak it to improve accuracy?\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download training and testing data\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "VZi8zCXJrav8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to experiment with batch size. (The number of images processed before updating the weights).\n",
        "\n",
        "As I understand it,\n",
        "Larger batch size : Faster training, lower accuracy\n",
        "Smaller batch size: Slower training, higher accuracy\n",
        "\n",
        "So I'm going to do some trial and error I think, to get the batch size at a point where I use up just under the whole 4 training hours, with the highest accuracy :)\n",
        "\n",
        "Contextualise: There are 50,000 training imgs, so I'm going to try a batch size of 100 for now? (As its a multiple, I assume it helps somehow?)\n",
        "UPDATED: Changed to 64, research suggests it helps with runtime to have it as a power of 2, for parallelism. Will take this into account for further optimisation."
      ],
      "metadata": {
        "id": "1fwoUvIGtKON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')"
      ],
      "metadata": {
        "id": "MgLYf_Y9uPpl"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I need to define the nn architecture/structure? I think I need to use a CNN rather than a regular NN because it's for images rather than numerical data"
      ],
      "metadata": {
        "id": "g1c8jRGF4Rz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "class MyCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # First layer? Has 3 input channels due to RGB\n",
        "    # input tensor = size of prev layer output * filters in prev layer\n",
        "    # output tensor = size of input img * filters\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=2, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels= 12,kernel_size=2, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=12*8*8, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n",
        "model = MyCNN().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkfUE5N64dPX",
        "outputId": "33ad379a-8513-41c7-efd1-970fde074f2f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyCNN(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 12, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=768, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "8qP-BYDG9P3o"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimiser):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "HfS2EQkC-5SR"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss, optimiser)\n",
        "  test(test_dataloader, model, loss)\n",
        "print(\"Done :)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSwKwo58_VLm",
        "outputId": "1c5e9499-e89b-42e1-bfed-410616b84c03"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "-------------------------------\n",
            "loss: 2.318991  [   64/50000]\n",
            "loss: 2.307722  [ 6464/50000]\n",
            "loss: 2.298464  [12864/50000]\n",
            "loss: 2.330194  [19264/50000]\n",
            "loss: 2.301045  [25664/50000]\n",
            "loss: 2.301604  [32064/50000]\n",
            "loss: 2.308650  [38464/50000]\n",
            "loss: 2.308275  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 11.2%, Avg loss: 2.299756 \n",
            "\n",
            "Epoch 2 \n",
            "-------------------------------\n",
            "loss: 2.297044  [   64/50000]\n",
            "loss: 2.303585  [ 6464/50000]\n",
            "loss: 2.302354  [12864/50000]\n",
            "loss: 2.297395  [19264/50000]\n",
            "loss: 2.288770  [25664/50000]\n",
            "loss: 2.293581  [32064/50000]\n",
            "loss: 2.296591  [38464/50000]\n",
            "loss: 2.290040  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 14.2%, Avg loss: 2.294048 \n",
            "\n",
            "Epoch 3 \n",
            "-------------------------------\n",
            "loss: 2.295617  [   64/50000]\n",
            "loss: 2.288590  [ 6464/50000]\n",
            "loss: 2.288798  [12864/50000]\n",
            "loss: 2.295392  [19264/50000]\n",
            "loss: 2.292320  [25664/50000]\n",
            "loss: 2.287771  [32064/50000]\n",
            "loss: 2.283040  [38464/50000]\n",
            "loss: 2.283222  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 14.5%, Avg loss: 2.286659 \n",
            "\n",
            "Epoch 4 \n",
            "-------------------------------\n",
            "loss: 2.278595  [   64/50000]\n",
            "loss: 2.291353  [ 6464/50000]\n",
            "loss: 2.286661  [12864/50000]\n",
            "loss: 2.283882  [19264/50000]\n",
            "loss: 2.287240  [25664/50000]\n",
            "loss: 2.284176  [32064/50000]\n",
            "loss: 2.294394  [38464/50000]\n",
            "loss: 2.281154  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 17.2%, Avg loss: 2.273490 \n",
            "\n",
            "Epoch 5 \n",
            "-------------------------------\n",
            "loss: 2.265847  [   64/50000]\n",
            "loss: 2.277384  [ 6464/50000]\n",
            "loss: 2.269225  [12864/50000]\n",
            "loss: 2.263141  [19264/50000]\n",
            "loss: 2.271101  [25664/50000]\n",
            "loss: 2.266692  [32064/50000]\n",
            "loss: 2.256384  [38464/50000]\n",
            "loss: 2.256191  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 19.9%, Avg loss: 2.249352 \n",
            "\n",
            "Done :)\n"
          ]
        }
      ]
    }
  ]
}