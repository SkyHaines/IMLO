{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVg4ALxfUKwk9t5JAWoVbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SkyHaines/IMLO/blob/main/Coursework%3F%3F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR 10 dataset contains 60,000 3x32x32 colour images, with 10 classes."
      ],
      "metadata": {
        "id": "-84D_fD6rDLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Need to look into how this works, perhaps i could tweak it to improve accuracy?\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Download training and testing data\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "VZi8zCXJrav8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b5a32d8-dd12-4a90-c5cc-f579e6bffa3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 52.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I need to experiment with batch size. (The number of images processed before updating the weights).\n",
        "\n",
        "As I understand it,\n",
        "Larger batch size : Faster training, lower accuracy\n",
        "Smaller batch size: Slower training, higher accuracy\n",
        "\n",
        "So I'm going to do some trial and error I think, to get the batch size at a point where I use up just under the whole 4 training hours, with the highest accuracy :)\n",
        "\n",
        "Contextualise: There are 50,000 training imgs, so I'm going to try a batch size of 100 for now? (As its a multiple, I assume it helps somehow?)\n",
        "UPDATED: Changed to 64, research suggests it helps with runtime to have it as a power of 2, for parallelism. Will take this into account for further optimisation."
      ],
      "metadata": {
        "id": "1fwoUvIGtKON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "classes = ('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')"
      ],
      "metadata": {
        "id": "MgLYf_Y9uPpl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I need to define the nn architecture/structure? I think I need to use a CNN rather than a regular NN because it's for images rather than numerical data"
      ],
      "metadata": {
        "id": "g1c8jRGF4Rz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "class MyCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # First layer? Has 3 input channels due to RGB\n",
        "    # input tensor = size of prev layer output * filters in prev layer\n",
        "    # output tensor = size of input img * filters\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=24, kernel_size=8, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=24, out_channels=48,kernel_size=4, stride=1, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=96*3*3, out_features=512)\n",
        "    self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
        "    self.fc3 = nn.Linear(in_features=256, out_features=128)\n",
        "    self.fc4 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.pool3(x)\n",
        "    x = torch.flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.fc4(x)\n",
        "    return x\n",
        "\n",
        "model = MyCNN().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkfUE5N64dPX",
        "outputId": "2b944e07-5966-468d-80d5-00444f4209a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyCNN(\n",
            "  (conv1): Conv2d(3, 24, kernel_size=(8, 8), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(24, 48, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=864, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "8qP-BYDG9P3o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimiser):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "\n",
        "      if batch % 100 == 0:\n",
        "        loss, current = loss.item(), (batch + 1) * len(X)\n",
        "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  model.eval()\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "HfS2EQkC-5SR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n-------------------------------\")\n",
        "  train(train_dataloader, model, loss, optimiser)\n",
        "  test(test_dataloader, model, loss)\n",
        "print(\"Done :)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qSwKwo58_VLm",
        "outputId": "04b9630f-733e-4d40-c400-fc73c04f2ce8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "-------------------------------\n",
            "loss: 1.657821  [   64/50000]\n",
            "loss: 1.879770  [ 6464/50000]\n",
            "loss: 1.899446  [12864/50000]\n",
            "loss: 1.932893  [19264/50000]\n",
            "loss: 1.881586  [25664/50000]\n",
            "loss: 1.596990  [32064/50000]\n",
            "loss: 1.779820  [38464/50000]\n",
            "loss: 1.582740  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 36.0%, Avg loss: 1.728957 \n",
            "\n",
            "Epoch 2 \n",
            "-------------------------------\n",
            "loss: 1.827741  [   64/50000]\n",
            "loss: 1.796122  [ 6464/50000]\n",
            "loss: 1.329588  [12864/50000]\n",
            "loss: 1.698887  [19264/50000]\n",
            "loss: 1.570302  [25664/50000]\n",
            "loss: 1.651444  [32064/50000]\n",
            "loss: 1.530965  [38464/50000]\n",
            "loss: 1.405856  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 42.0%, Avg loss: 1.575529 \n",
            "\n",
            "Epoch 3 \n",
            "-------------------------------\n",
            "loss: 1.497724  [   64/50000]\n",
            "loss: 1.689972  [ 6464/50000]\n",
            "loss: 1.360698  [12864/50000]\n",
            "loss: 1.616765  [19264/50000]\n",
            "loss: 1.437613  [25664/50000]\n",
            "loss: 1.506308  [32064/50000]\n",
            "loss: 1.758958  [38464/50000]\n",
            "loss: 1.337045  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 45.8%, Avg loss: 1.488845 \n",
            "\n",
            "Epoch 4 \n",
            "-------------------------------\n",
            "loss: 1.224374  [   64/50000]\n",
            "loss: 1.407850  [ 6464/50000]\n",
            "loss: 1.408749  [12864/50000]\n",
            "loss: 1.422328  [19264/50000]\n",
            "loss: 1.292683  [25664/50000]\n",
            "loss: 1.544088  [32064/50000]\n",
            "loss: 1.493294  [38464/50000]\n",
            "loss: 1.223253  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 44.8%, Avg loss: 1.510785 \n",
            "\n",
            "Epoch 5 \n",
            "-------------------------------\n",
            "loss: 1.352378  [   64/50000]\n",
            "loss: 1.353841  [ 6464/50000]\n",
            "loss: 1.510821  [12864/50000]\n",
            "loss: 1.330317  [19264/50000]\n",
            "loss: 1.435001  [25664/50000]\n",
            "loss: 1.240223  [32064/50000]\n",
            "loss: 1.342212  [38464/50000]\n",
            "loss: 1.201892  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 50.2%, Avg loss: 1.363238 \n",
            "\n",
            "Epoch 6 \n",
            "-------------------------------\n",
            "loss: 1.349172  [   64/50000]\n",
            "loss: 1.460672  [ 6464/50000]\n",
            "loss: 1.318283  [12864/50000]\n",
            "loss: 1.394510  [19264/50000]\n",
            "loss: 1.217337  [25664/50000]\n",
            "loss: 1.376347  [32064/50000]\n",
            "loss: 1.263076  [38464/50000]\n",
            "loss: 1.274387  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 50.9%, Avg loss: 1.362976 \n",
            "\n",
            "Epoch 7 \n",
            "-------------------------------\n",
            "loss: 1.187484  [   64/50000]\n",
            "loss: 1.154688  [ 6464/50000]\n",
            "loss: 1.304020  [12864/50000]\n",
            "loss: 1.379338  [19264/50000]\n",
            "loss: 1.210236  [25664/50000]\n",
            "loss: 1.397224  [32064/50000]\n",
            "loss: 1.150110  [38464/50000]\n",
            "loss: 1.030472  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 49.6%, Avg loss: 1.446793 \n",
            "\n",
            "Epoch 8 \n",
            "-------------------------------\n",
            "loss: 1.251994  [   64/50000]\n",
            "loss: 1.326897  [ 6464/50000]\n",
            "loss: 1.186347  [12864/50000]\n",
            "loss: 1.171478  [19264/50000]\n",
            "loss: 1.309181  [25664/50000]\n",
            "loss: 1.274680  [32064/50000]\n",
            "loss: 1.033507  [38464/50000]\n",
            "loss: 1.164054  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 57.1%, Avg loss: 1.221559 \n",
            "\n",
            "Epoch 9 \n",
            "-------------------------------\n",
            "loss: 1.204522  [   64/50000]\n",
            "loss: 1.213604  [ 6464/50000]\n",
            "loss: 1.000574  [12864/50000]\n",
            "loss: 1.131916  [19264/50000]\n",
            "loss: 0.948119  [25664/50000]\n",
            "loss: 1.463015  [32064/50000]\n",
            "loss: 1.206356  [38464/50000]\n",
            "loss: 1.226709  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 44.4%, Avg loss: 1.640458 \n",
            "\n",
            "Epoch 10 \n",
            "-------------------------------\n",
            "loss: 1.600190  [   64/50000]\n",
            "loss: 0.906484  [ 6464/50000]\n",
            "loss: 1.051491  [12864/50000]\n",
            "loss: 1.003770  [19264/50000]\n",
            "loss: 1.216342  [25664/50000]\n",
            "loss: 1.112799  [32064/50000]\n",
            "loss: 0.742662  [38464/50000]\n",
            "loss: 1.437967  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 49.9%, Avg loss: 1.557593 \n",
            "\n",
            "Epoch 11 \n",
            "-------------------------------\n",
            "loss: 1.360437  [   64/50000]\n",
            "loss: 1.238680  [ 6464/50000]\n",
            "loss: 0.895227  [12864/50000]\n",
            "loss: 1.102438  [19264/50000]\n",
            "loss: 1.002455  [25664/50000]\n",
            "loss: 1.063323  [32064/50000]\n",
            "loss: 1.316792  [38464/50000]\n",
            "loss: 1.219094  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 55.8%, Avg loss: 1.279222 \n",
            "\n",
            "Epoch 12 \n",
            "-------------------------------\n",
            "loss: 1.172751  [   64/50000]\n",
            "loss: 0.909171  [ 6464/50000]\n",
            "loss: 1.110841  [12864/50000]\n",
            "loss: 1.038016  [19264/50000]\n",
            "loss: 0.976586  [25664/50000]\n",
            "loss: 1.044958  [32064/50000]\n",
            "loss: 1.224610  [38464/50000]\n",
            "loss: 1.000547  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 62.3%, Avg loss: 1.079917 \n",
            "\n",
            "Epoch 13 \n",
            "-------------------------------\n",
            "loss: 0.742797  [   64/50000]\n",
            "loss: 0.889412  [ 6464/50000]\n",
            "loss: 1.090284  [12864/50000]\n",
            "loss: 1.165971  [19264/50000]\n",
            "loss: 0.693477  [25664/50000]\n",
            "loss: 1.087993  [32064/50000]\n",
            "loss: 0.973962  [38464/50000]\n",
            "loss: 1.272825  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 64.6%, Avg loss: 1.019931 \n",
            "\n",
            "Epoch 14 \n",
            "-------------------------------\n",
            "loss: 0.765664  [   64/50000]\n",
            "loss: 1.047108  [ 6464/50000]\n",
            "loss: 1.019987  [12864/50000]\n",
            "loss: 0.999713  [19264/50000]\n",
            "loss: 0.954098  [25664/50000]\n",
            "loss: 1.059937  [32064/50000]\n",
            "loss: 0.942418  [38464/50000]\n",
            "loss: 0.768186  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 63.0%, Avg loss: 1.072550 \n",
            "\n",
            "Epoch 15 \n",
            "-------------------------------\n",
            "loss: 0.986090  [   64/50000]\n",
            "loss: 0.768606  [ 6464/50000]\n",
            "loss: 0.789335  [12864/50000]\n",
            "loss: 0.936837  [19264/50000]\n",
            "loss: 1.020253  [25664/50000]\n",
            "loss: 1.205839  [32064/50000]\n",
            "loss: 0.833768  [38464/50000]\n",
            "loss: 0.926928  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 62.1%, Avg loss: 1.084971 \n",
            "\n",
            "Epoch 16 \n",
            "-------------------------------\n",
            "loss: 0.880102  [   64/50000]\n",
            "loss: 0.688599  [ 6464/50000]\n",
            "loss: 0.653141  [12864/50000]\n",
            "loss: 0.890307  [19264/50000]\n",
            "loss: 0.780500  [25664/50000]\n",
            "loss: 1.028320  [32064/50000]\n",
            "loss: 0.725720  [38464/50000]\n",
            "loss: 0.957279  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 59.5%, Avg loss: 1.204376 \n",
            "\n",
            "Epoch 17 \n",
            "-------------------------------\n",
            "loss: 1.235131  [   64/50000]\n",
            "loss: 0.860553  [ 6464/50000]\n",
            "loss: 0.773303  [12864/50000]\n",
            "loss: 0.775002  [19264/50000]\n",
            "loss: 0.926319  [25664/50000]\n",
            "loss: 1.041694  [32064/50000]\n",
            "loss: 0.616025  [38464/50000]\n",
            "loss: 0.702810  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 65.0%, Avg loss: 1.010867 \n",
            "\n",
            "Epoch 18 \n",
            "-------------------------------\n",
            "loss: 0.845671  [   64/50000]\n",
            "loss: 0.552480  [ 6464/50000]\n",
            "loss: 1.053784  [12864/50000]\n",
            "loss: 0.690982  [19264/50000]\n",
            "loss: 0.804725  [25664/50000]\n",
            "loss: 0.779512  [32064/50000]\n",
            "loss: 0.740165  [38464/50000]\n",
            "loss: 0.708139  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 64.6%, Avg loss: 1.026804 \n",
            "\n",
            "Epoch 19 \n",
            "-------------------------------\n",
            "loss: 0.806361  [   64/50000]\n",
            "loss: 0.966063  [ 6464/50000]\n",
            "loss: 1.066810  [12864/50000]\n",
            "loss: 0.662600  [19264/50000]\n",
            "loss: 0.810412  [25664/50000]\n",
            "loss: 0.661849  [32064/50000]\n",
            "loss: 1.001061  [38464/50000]\n",
            "loss: 1.054402  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 63.3%, Avg loss: 1.102038 \n",
            "\n",
            "Epoch 20 \n",
            "-------------------------------\n",
            "loss: 0.819255  [   64/50000]\n",
            "loss: 0.705508  [ 6464/50000]\n",
            "loss: 0.723952  [12864/50000]\n",
            "loss: 0.648010  [19264/50000]\n",
            "loss: 0.871527  [25664/50000]\n",
            "loss: 0.815350  [32064/50000]\n",
            "loss: 0.640237  [38464/50000]\n",
            "loss: 0.656540  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 63.2%, Avg loss: 1.087046 \n",
            "\n",
            "Epoch 21 \n",
            "-------------------------------\n",
            "loss: 0.952323  [   64/50000]\n",
            "loss: 0.694521  [ 6464/50000]\n",
            "loss: 0.650788  [12864/50000]\n",
            "loss: 0.697580  [19264/50000]\n",
            "loss: 0.679152  [25664/50000]\n",
            "loss: 0.703605  [32064/50000]\n",
            "loss: 0.757540  [38464/50000]\n",
            "loss: 0.601789  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 68.9%, Avg loss: 0.914997 \n",
            "\n",
            "Epoch 22 \n",
            "-------------------------------\n",
            "loss: 0.640623  [   64/50000]\n",
            "loss: 0.679296  [ 6464/50000]\n",
            "loss: 0.757127  [12864/50000]\n",
            "loss: 0.867510  [19264/50000]\n",
            "loss: 0.591111  [25664/50000]\n",
            "loss: 0.719211  [32064/50000]\n",
            "loss: 0.771681  [38464/50000]\n",
            "loss: 0.575158  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 65.2%, Avg loss: 1.038421 \n",
            "\n",
            "Epoch 23 \n",
            "-------------------------------\n",
            "loss: 1.052750  [   64/50000]\n",
            "loss: 0.625304  [ 6464/50000]\n",
            "loss: 0.684072  [12864/50000]\n",
            "loss: 0.632562  [19264/50000]\n",
            "loss: 0.603394  [25664/50000]\n",
            "loss: 0.618513  [32064/50000]\n",
            "loss: 0.846851  [38464/50000]\n",
            "loss: 0.516054  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 63.6%, Avg loss: 1.143233 \n",
            "\n",
            "Epoch 24 \n",
            "-------------------------------\n",
            "loss: 0.690636  [   64/50000]\n",
            "loss: 0.684007  [ 6464/50000]\n",
            "loss: 0.746151  [12864/50000]\n",
            "loss: 0.585656  [19264/50000]\n",
            "loss: 0.581436  [25664/50000]\n",
            "loss: 0.504531  [32064/50000]\n",
            "loss: 0.737043  [38464/50000]\n",
            "loss: 0.630910  [44864/50000]\n",
            "Test error: \n",
            " Accuracy: 60.8%, Avg loss: 1.319667 \n",
            "\n",
            "Epoch 25 \n",
            "-------------------------------\n",
            "loss: 1.224985  [   64/50000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-52d31506c2b5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1} \\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done :)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5d03f4a6cbc1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimiser)\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-bf302fa948a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}